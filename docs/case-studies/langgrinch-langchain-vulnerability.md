# LangGrinch - LangChain Serialization Vulnerability (CVE-2025-68664) - December 2025

## Incident Overview

**Vulnerability**: CVE-2025-68664 ("LangGrinch")<br>
**Affected Software**: LangChain Core (langchain-core)<br>
**Date Discovered**: December 4, 2025<br>
**Advisory Published**: December 23, 2025<br>
**Failure Mode**: [Prompt Injection](../failure-modes/prompt-injection.md) + [Incorrect Tool Use](../failure-modes/tool-use.md)<br>
**CVSS Score**: 9.3 (Critical)<br>
**Impact**: Secret exfiltration from environment variables, arbitrary object instantiation, potential RCE via Jinja2 templates 

## What Happened

Security researcher Yarden Porat discovered a critical serialization vulnerability in LangChain's core library that could allow attackers to extract secrets and execute arbitrary code through AI agent workflows. The vulnerability was dubbed "LangGrinch" and affects any LangChain orchestration loop that serializes and later deserializes user-controlled content.

### The Attack Vector

The vulnerability exploits LangChain's internal serialization format, where dictionaries containing an `lc` marker represent LangChain objects. The core issue: the `dumps()` and `dumpd()` APIs did not properly escape user-controlled dictionaries that happened to include the reserved `lc` key.

When an attacker can make a LangChain orchestration loop serialize and later deserialize content including an `lc` key, they can instantiate arbitrary objects within pre-approved trusted namespaces.

## Technical Details

### Vulnerable Code Pattern

The vulnerability exists in the serialization path, not the deserialization path. In agent frameworks, structured data produced downstream of a prompt is often persisted, streamed, and reconstructed later. This creates a large attack surface reachable from a single prompt.

### Attack Outcomes

1. **Secret Extraction**: When deserialization is performed with `secrets_from_env=True` (previously the default), attackers could extract sensitive credentials and API keys from environment variables

2. **Arbitrary Object Instantiation**: Attackers could instantiate classes within pre-approved trusted namespaces

3. **Remote Code Execution**: Potential RCE via Jinja2 template injection when templates are not blocked

### Affected Versions

- All versions of langchain-core prior to 1.2.5 (for 1.x branch)
- All versions prior to 0.3.81 (for 0.3.x branch)

### Related Vulnerability

LangChainJS also had a related advisory (GHSA-r399-636x-v7f6 / CVE-2025-68665) with similar mechanics: `lc` marker confusion during serialization enabling secret extraction and unsafe instantiation.

## Root Cause Analysis

### AI Agent Architecture Failures

1. **Unsafe Default Configuration**
   - `secrets_from_env=True` was the default, enabling automatic secret loading
   - Jinja2 templates were not blocked by default
   - No allowlist for deserializable objects

2. **Serialization Path Vulnerability**
   - User-controlled data could contain reserved markers
   - No input sanitization for internal serialization keys
   - Trust boundary confusion between user data and system data

3. **Attack Surface in Agent Workflows**
   - Agent orchestration loops commonly persist and reconstruct data
   - Multiple serialization/deserialization cycles amplify risk
   - Downstream data treated as trusted despite user influence

## The Fix

LangChain released patches in versions 1.2.5 and 0.3.81 with the following changes:

```python
# New restrictive defaults in load() and loads()
load(
    data,
    allowed_objects=["explicit", "allowlist", "of", "classes"],  # New parameter
    secrets_from_env=False,  # Changed from True
    # Jinja2 templates blocked by default
)
```

### Key Security Improvements

1. **Allowlist Parameter**: Users must explicitly specify which classes can be serialized/deserialized
2. **Secrets Disabled by Default**: `secrets_from_env` now defaults to `False`
3. **Jinja2 Blocking**: Templates blocked by default to prevent code execution

## Industry Recommendations

### For LangChain Users

1. **Update Immediately**: Upgrade to langchain-core >= 1.2.5 or >= 0.3.81
2. **Audit Workflows**: Review any workflows that serialize/deserialize user-influenced data
3. **Minimize Secrets in Environment**: Reduce secrets accessible via environment variables
4. **Explicit Allowlists**: Use the new `allowed_objects` parameter explicitly

### For Agent Framework Developers

1. **Secure Defaults**: Security-critical options should default to the safe option
2. **Input Sanitization**: Sanitize reserved markers in user-controlled data
3. **Trust Boundaries**: Clearly define and enforce trust boundaries in serialization
4. **Audit Serialization Paths**: Serialization vulnerabilities are often overlooked

### Code Example: Secure Configuration

```python
from langchain_core.load import load

# Secure configuration - explicit allowlist, no auto-secrets
result = load(
    serialized_data,
    allowed_objects=["langchain_core.messages.HumanMessage"],
    secrets_from_env=False
)
```

## Lessons Learned

### For AI Agent Developers
1. **Serialization is an Attack Surface**: Any serialization/deserialization cycle can be exploited
2. **Defaults Matter**: Secure defaults prevent vulnerabilities from reaching production
3. **User Data is Untrusted**: Even structured data influenced by users requires sanitization

### For Security Teams
1. **Agent Frameworks Need Security Review**: Complex orchestration creates novel attack vectors
2. **Monitor for CVEs**: Agent frameworks are actively targeted by security researchers
3. **Dependency Management**: Quickly patch critical vulnerabilities in agent dependencies

### For Organizations
1. **Secrets Management**: Minimize secrets in environment variables; use secret managers
2. **Update Policies**: Have rapid update procedures for critical security patches
3. **Defense in Depth**: Don't rely solely on framework security; add additional layers

## References

- **Primary Coverage**: [Cyata - LangGrinch hits LangChain Core](https://cyata.ai/blog/langgrinch-langchain-core-cve-2025-68664/)
- **Security Advisory**: [TheHackerNews - Critical LangChain Core Vulnerability](https://thehackernews.com/2025/12/critical-langchain-core-vulnerability.html)
- **NVD Entry**: [CVE-2025-68664](https://nvd.nist.gov/vuln/detail/CVE-2025-68664)
- **Technical Analysis**: [Security Affairs - LangChain core vulnerability](https://securityaffairs.com/186185/hacking/langchain-core-vulnerability-allows-prompt-injection-and-data-exposure.html)
- **Architecture Analysis**: [Amla Labs - LangGrinch: A Bug in the Library](https://amlalabs.com/blog/langgrinch-cve-2025-68664/)
- **Industry Impact**: [SiliconANGLE - Critical LangGrinch vulnerability](https://siliconangle.com/2025/12/25/critical-langgrinch-vulnerability-langchain-core-puts-ai-agent-secrets-risk/)
