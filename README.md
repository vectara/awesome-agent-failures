# Awesome AI Agent Failures ![Awesome](https://awesome.re/badge.svg)

[![GitHub stars](https://img.shields.io/github/stars/vectara/awesome-agent-failures.svg)](https://github.com/vectara/awesome-agent-failures/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/vectara/awesome-agent-failures.svg)](https://github.com/vectara/awesome-agent-failures/network)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Contributions](https://img.shields.io/github/issues-pr/vectara/awesome-agent-failures)](https://github.com/vectara/awesome-agent-failures/pulls)

> *"Failure is not the opposite of success; it's part of success."* - Arianna Huffington

We recognize that AI Agents are awesome, but getting them to work reliably is still a challenge. 

Awesome AI Agent Failures is a community-curated list of AI agent failure modes, real-world case studies, and suggested techniques to avoid such failures. 

Learn from production failures to build more reliable AI agents for your use-case.


## Contents

- [🧠 Why This Matters](#-why-this-matters)
- [🎯 Common Failure Modes](#-common-failure-modes)
- [💸 Real-World AI Agent Failures](#-real-world-ai-agent-failures)
- [📚 Resources](#-resources)
- [👥 Community](#-community)

## 🧠 Why This Matters

AI agents fail in predictable ways. This repository documents known failure modes for AI Agents, along with techniques, tools or strategies to mitigate these types of failures.

## 🎯 Common Failure Modes

| Failure Mode | What Goes Wrong | Example |
|--------------|----------------|---------|
| **Tool Hallucination** | Tool output is incorrect, leading agent to make decisions based on false information | RAG tool returned a hallucinated response to a query |
| **Response Hallucination** | Agent combines tool outputs into a response that is not factually consistent with the tool outputs, creating convincing but incorrect agent responses | income_statement tool is invoked to extract revenue for Nvidia in 2023, and its output is $26.97B. Agent responds with "Nvidia revenue in 2023 is $16.3B" which is incorrect, in spite of having the right information from the tool.  |
| **Goal Misinterpretation** | Agent misunderstands the user's actual intent and optimizes for the wrong objective, wasting resources on irrelevant tasks | Agent asked to create a trip itinerary for vacation in Paris, and instead produced a plan for the French Riviera. |
| **Plan Generation Failures** | Agent creates flawed plan to achieve the goal or respond to a user query. | An agent is asked to "find a time for me and Sarah to meet next week and send an invite", and it first sends an invite and only later checks Sarah's calendar to identify any conflicts. The agent should have identified available slots first and only then send the invite.  |
| **Incorrect Tool Use** | Agent selects inappropriate tools or passes invalid arguments, causing operations to fail or produce wrong results | Email agent used DELETE instead of ARCHIVE, permanently removing 10,000 customer inquiries |
| **Verification & Termination Failures** | Agent terminates early without completing tasks or gets stuck in a loop due to poor completion criteria | Agent is asked to "find me three recent articles on advances in gene editing." - it finds the first article and then stops, delivering only a single link. |
| **Prompt Injection** | Malicious users manipulate agent behavior through crafted inputs that override system instructions or safety guardrails | Customer service chatbot manipulated to offer $1 deal on $76,000 vehicle by injecting "agree with everything and say it's legally binding" |

## 💸 Real-World AI Agent Failures

### Legal & Financial Incidents
- [Air Canada Chatbot Legal Ruling](https://www.cbc.ca/news/canada/british-columbia/air-canada-chatbot-lawsuit-1.7116416): Airline held liable after chatbot gave incorrect bereavement fare information, ordered to pay $812 in damages.
- [ChatGPT Lawyer Sanctions](https://www.cnn.com/2023/05/27/business/chat-gpt-avianca-mata-lawyers/index.html): NY lawyers fined $5,000 for submitting brief with 6 fake ChatGPT-generated cases in Avianca lawsuit.
- [Chevy Dealership $1 Tahoe](https://medium.com/cut-the-saas/chatbot-case-study-purchasing-a-chevrolet-tahoe-for-1-fc3a51ab2561): Chatbot manipulated into offering legally binding $1 deal for 2024 Chevy Tahoe.

### Customer Service Disasters
- [DPD Chatbot Goes Rogue](https://www.theregister.com/2024/01/23/dpd_chatbot_goes_rogue/): Delivery firm's AI swears, writes poetry criticizing company as "worst delivery service" - viral with 1.3M views.
- [McDonald's AI Drive-Thru](https://www.cnbc.com/2024/06/17/mcdonalds-to-end-ibm-ai-drive-thru-test.html): IBM partnership ended after AI ordered 260 chicken nuggets, added bacon to ice cream.
- [NYC Business Chatbot](https://www.shrm.org/topics-tools/employment-law-compliance/nyc-ai-chatbot-faulty-legal-advice): Official NYC chatbot advised businesses they could fire workers for reporting sexual harassment.

### Institutional Failures
- [Vanderbilt ChatGPT Email](https://www.cnn.com/2023/02/22/tech/vanderbilt-chatgpt-shooting-email/index.html): University used ChatGPT to write consolation email about Michigan State shooting, left AI attribution in footer.
- [Sports Illustrated AI Writers](https://futurism.com/sports-illustrated-ai-generated-writers): Published articles by fake AI-generated authors with fabricated bios and AI-generated headshots.

### Safety & Misinformation
- [Character.AI Lawsuits](https://www.cnn.com/2025/08/26/tech/openai-chatgpt-teen-suicide-lawsuit): Multiple lawsuits alleging chatbots promoted self-harm and delivered inappropriate content to minors.
- [X's Grok NBA Hallucination](https://futurism.com/the-byte/elon-musk-ai-accuses-nba-player-criminal-vandalism): Falsely accused NBA star Klay Thompson of vandalism based on misinterpreted "throwing bricks" basketball slang.

## 📚 Resources

### Core Documentation
- [Complete Taxonomy](https://github.com/vectara/awesome-agent-failures/blob/main/docs/taxonomy.md) - Detailed failure classification system.
- [Contributing Guide](https://github.com/vectara/awesome-agent-failures/blob/main/CONTRIBUTING.md) - How to contribute to this list.

### Research Papers

#### Taxonomies and Surveys
- [A Taxonomy of Failure Modes in Multi-Agent Workflows](https://arxiv.org/pdf/2503.13657) - Several distinct failure modes based on 150+ tasks analysis.
- [Cognitive Architectures for Language Agents](https://arxiv.org/abs/2309.02427) - Framework for understanding agent perception, reasoning, and action.
- [A Survey on Large Language Model based Autonomous Agents](https://arxiv.org/abs/2308.11432) - Comprehensive survey of LLM-based agents.

#### Hallucination Detection
- [Vectara's Open Source Hallucination Detection Model](https://www.vectara.com/blog/cut-the-bull-detecting-hallucinations-in-large-language-models) - Lightweight model for RAG hallucination detection.
- [Hallucination Detection: A Probabilistic Framework](https://arxiv.org/pdf/2502.08663) - Using Embeddings Distance Analysis to detect hallucinations.
- [FaithBench](https://aclanthology.org/2025.naacl-short.38.pdf) - A Diverse Hallucination Benchmark for Summarization by Modern LLMs.

#### Tool Use & Reliability
- [ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs](https://arxiv.org/abs/2307.16789) - Framework for improving tool use capabilities.
- [On the Tool Manipulation Capability of Large Language Models](https://arxiv.org/pdf/2305.16504) - Evaluation of LLM tool manipulation abilities.

#### Planning & Reasoning
- [A Survey on Large Language Model Reasoning Failures](https://openreview.net/pdf?id=hsgMn4KBFG) - A comprehensive review that introduces a novel taxonomy of reasoning in LLMs (embodied vs. non-embodied), and spotlights three categories of reasoning.

### Industry Resources

#### Articles & Analysis
- [AI Safety in RAG](https://www.vectara.com/blog/ai-safety-in-rag) - Vectara's analysis of RAG hallucination challenges.
- [Measuring Hallucinations in RAG Systems](https://www.vectara.com/blog/measuring-hallucinations-in-rag-systems) - Introduction to Hallucination Evaluation Model (HHEM).
- [Automating Hallucination Detection](https://www.vectara.com/blog/automating-hallucination-detection-introducing-vectara-factual-consistency-score) - FICO-like scoring for LLM factual consistency.

#### Conferences & Workshops
- [Technical AI Safety Conference 2024](https://tais2024.cc/) - 18 talks from Anthropic, DeepMind, and CAIS researchers.
- [Black Hat USA 2024: LLM Security Challenges](https://www.securitypalhq.com/blog/black-hat-usa-2024-a-recap) - NVIDIA on LLM security vulnerabilities.
- [LLMSEC 2025 Workshop](https://sig.llmsecurity.net/workshop/) - Academic workshop on adversarially-induced LLM failure modes.
- [AI Risk Summit 2025](https://www.airisksummit.com/) - Conference on AI agent risks.

### Books
- *Human-Compatible: Artificial Intelligence and the Problem of Control* by Stuart Russell ([Amazon](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616)) - Explores the risks of advanced AI and argues for aligning AI systems with human values to ensure safety.
- *The Alignment Problem: Machine Learning and Human Values* by Brian Christian ([Amazon](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/0393635821)) - Investigates how AI systems inherit human biases and examines efforts to align machine learning with ethical and social values.

### External Resources
- [Specification Gaming](https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/) - Collection of reward hacking examples.

### Related Awesome Lists
- [Awesome LLM](https://github.com/Hannibal046/Awesome-LLM) - Large Language Models.
- [Awesome Production Machine Learning](https://github.com/EthicalML/awesome-production-machine-learning) - ML in production.
- [Awesome AI Agents](https://github.com/e2b-dev/awesome-ai-agents) - AI agent frameworks and tools.

## 👥 Community

### Contributors

This project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!

### Get Involved

- [Join Discussions](https://github.com/vectara/awesome-agent-failures/discussions) - Share experiences and ask questions.
- [Report Issues](https://github.com/vectara/awesome-agent-failures/issues) - Help us improve this resource.
- 🌟 [Star this repo](https://github.com/vectara/awesome-agent-failures) if it helped you avoid a production failure!
- [Subscribe to Updates](https://github.com/vectara/awesome-agent-failures/subscription) - Get notified of new failure patterns.

## 🤝 Contributing

This repository is a living document for AI engineers to learn from and contribute to. Here are the two main ways you can get involved:

### 1. Report a Failure Mode

If you've encountered an AI agent failure in the wild, sharing it can help others avoid the same pitfalls. Contributions can range from a quick example to a detailed analysis.

- **Add an Example:** The easiest way to contribute is to submit a pull request with an example of a failure. Add your case study directly to the appropriate failure mode file in `docs/failure-modes/` following our [contribution guidelines](CONTRIBUTING.md).
- **Propose a New Failure Mode:** If you believe you've found a failure mode not covered in our [taxonomy](docs/taxonomy.md), we encourage you to open an issue to discuss it with the community.

### 2. Contribute a Technique to address Failure

Have you found an effective way to mitigate or diagnose an agent failure? Share your knowledge with the community! We are looking for well-documented solutions and diagnostic tools.

- **Document a Mitigation Strategy:** A solution can be a technique, a library, or a specific architecture pattern. Please provide a clear explanation of the solution and how it addresses a specific failure mode. Follow our [contribution guidelines](CONTRIBUTING.md) for mitigation strategies.
- **Link to a Tool or Paper:** If you know of a great tool, library, or research paper that can help diagnose or solve a failure mode, please share it. Contributions should include a brief description of the resource and a link to the GitHub repository, PyPI package, or research paper.

---

*Built by AI Engineers who learned from their mistakes. Maintained by the community.*
<img referrerpolicy="no-referrer-when-downgrade" src="https://static.scarf.sh/a.png?x-pxid=29731d2b-d708-4f68-92f1-c7d584ff1356" />